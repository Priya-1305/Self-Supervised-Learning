{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Self-Supervised Learning with SimSiam on CIFAR-10\n",
        "**Zero labels during pre-training â†’ ~91% accuracy!**\n",
        "\n",
        "Day 11 of 100 Days of AI/ML | Runs in ~40 mins on free Colab T4\n",
        "\n",
        "Author: [Your Name] | GitHub: [your-github-link]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 1. Dual Augmentation Transform"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DualAugment:\n",
        "    def __init__(self):\n",
        "        self.aug = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                                 std=[0.2023, 0.1994, 0.2010]),\n",
        "        ])\n",
        "    def __call__(self, x):\n",
        "        return self.aug(x), self.aug(x)\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True,\n",
        "                        transform=DualAugment())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 2. ResNet-18 Backbone"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_c != out_c:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_c)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.l1 = nn.Sequential(BasicBlock(64,64), BasicBlock(64,64))\n",
        "        self.l2 = nn.Sequential(BasicBlock(64,128,2), BasicBlock(128,128))\n",
        "        self.l3 = nn.Sequential(BasicBlock(128,256,2), BasicBlock(256,256))\n",
        "        self.l4 = nn.Sequential(BasicBlock(256,512,2), BasicBlock(512,512))\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.l1(x); x = self.l2(x); x = self.l3(x); x = self.l4(x)\n",
        "        x = nn.functional.avg_pool2d(x, 4)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 3. SimSiam Model + Loss"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(512, 2048, bias=False), nn.BatchNorm1d(2048), nn.ReLU(),\n",
        "            nn.Linear(2048, 2048, bias=False), nn.BatchNorm1d(2048), nn.ReLU(),\n",
        "            nn.Linear(2048, 2048, bias=False), nn.BatchNorm1d(2048, affine=False)\n",
        "        )\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Linear(512, 2048)\n",
        "        )\n",
        "    def forward(self, x1, x2):\n",
        "        z1, z2 = self.projection(self.backbone(x1)), self.projection(self.backbone(x2))\n",
        "        p1, p2 = self.predictor(z1), self.predictor(z2)\n",
        "        return p1, p2, z1.detach(), z2.detach()\n",
        "\n",
        "def loss_fn(p, z):\n",
        "    p = nn.functional.normalize(p, dim=1)\n",
        "    z = nn.functional.normalize(z, dim=1)\n",
        "    return -(p * z).sum(dim=1).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
